<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Feature Pyramid</title>
    <url>/2021/12/20/FeaturePyramid/</url>
    <content><![CDATA[<h2 id="Feature-Pyramid-amp-amp-GAN"><a href="#Feature-Pyramid-amp-amp-GAN" class="headerlink" title="Feature Pyramid &amp;&amp; GAN"></a>Feature Pyramid &amp;&amp; GAN</h2><h3 id="An-Unsupervised-Learning-Based-Approach-for-Automated-Defect-Inspection-on-Textured-Surfaces-2018-91"><a href="#An-Unsupervised-Learning-Based-Approach-for-Automated-Defect-Inspection-on-Textured-Surfaces-2018-91" class="headerlink" title="An Unsupervised-Learning-Based Approach for Automated Defect Inspection on Textured Surfaces[2018, 91]"></a>An Unsupervised-Learning-Based Approach for Automated Defect Inspection on Textured Surfaces[2018, 91]</h3><p>像素级（8x8邻域作为patch进行重构，得到残差—-全局特征？）二分类任务<br><span id="more"></span><br>网络结构(来源：<a href="https://blog.csdn.net/hhy_csdn/article/details/84679448#commentBox">https://blog.csdn.net/hhy_csdn/article/details/84679448#commentBox</a>)<br><img src="https://img-blog.csdnimg.cn/20181203175011695.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hoeV9jc2Ru,size_16,color_FFFFFF,t_70" alt="avatar"></p>
<h3 id="同组论文-Multiscale-Feature-Clustering-Based-Fully-Convolutional-Autoencoder-for-Fast-Accurate-Visual-Inspection-of-Texture-Surface-Defects-2019-33"><a href="#同组论文-Multiscale-Feature-Clustering-Based-Fully-Convolutional-Autoencoder-for-Fast-Accurate-Visual-Inspection-of-Texture-Surface-Defects-2019-33" class="headerlink" title="(同组论文)Multiscale Feature-Clustering-Based Fully Convolutional Autoencoder for Fast Accurate Visual Inspection of Texture Surface Defects[2019, 33]"></a>(同组论文)Multiscale Feature-Clustering-Based Fully Convolutional Autoencoder for Fast Accurate Visual Inspection of Texture Surface Defects[2019, 33]</h3><h4 id="MS-FCAE-的总体架构"><a href="#MS-FCAE-的总体架构" class="headerlink" title="MS-FCAE 的总体架构"></a>MS-FCAE 的总体架构</h4><p><img src="/2021/12/20/FeaturePyramid/MS-FCAE.png" alt="MS-FCAE"></p>
<p>MS-FCAE有特征提取网络（几个卷积层）、多尺度FCAE网络哦和结果融合模块做成；<br>FCAE子网由一个编码模块、一个特征聚合模块和一个解码模块组成。</p>
<p><img src="/2021/12/20/FeaturePyramid/FCAE.png" alt="FCAE"></p>
]]></content>
      <categories>
        <category>PaperNotes</category>
      </categories>
      <tags>
        <tag>Feature Pyramid</tag>
      </tags>
  </entry>
  <entry>
    <title>Swin Transformer</title>
    <url>/2021/12/30/SwinTransformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows-CVPR-2021-Best-Paper-Mcrosoft"><a href="#Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows-CVPR-2021-Best-Paper-Mcrosoft" class="headerlink" title="Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(CVPR 2021 Best Paper)(Mcrosoft)"></a><a href="https://github.com/microsoft/Swin-Transformer">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(CVPR 2021 Best Paper)(Mcrosoft)</a></h2><blockquote>
<p>新的计算机视觉网络backbone<br>图像域和语言域的差异</p>
</blockquote>
<p>Swin Transformer提出的分层tranformer使用Shifted Windows（滑动窗口）计算特征图。Shifted Windows 方案通过将自注意力计算限制在不重叠的本地窗口同时允许跨窗口连接来提高效率。<br><span id="more"></span></p>
<h3 id="Swin-Transformer的网络结构"><a href="#Swin-Transformer的网络结构" class="headerlink" title="Swin Transformer的网络结构"></a>Swin Transformer的网络结构</h3><p><img src="/2021/12/30/SwinTransformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/transformer_architecture.png" alt="swin transformer architecture"></p>
<ul>
<li><strong>patch partition</strong>: 将图像分割成不重叠的系列块；</li>
</ul>
<blockquote>
<p>its feature is et as a concatenation of the raw pixel RGB values.??</p>
</blockquote>
<ul>
<li><strong>linear embedding</strong>将raw-value的特征映射到固定维度的空间上；</li>
<li><strong>swin transformer blocks</strong>应用与patch tokens，实现feature transformation;通过将标准的multi-head self attention(MSA)替换为基于shifted windows 的tranformer block构建成。</li>
<li><strong>patch merging</strong>减少tokens 的数量以产生分层的表征</li>
</ul>
<h3 id="Shifted-Window-based-Self-Attention"><a href="#Shifted-Window-based-Self-Attention" class="headerlink" title="Shifted Window based Self-Attention"></a>Shifted Window based Self-Attention</h3><blockquote>
<p><strong><a href="https://paperswithcode.com/method/gelu">GELU</a></strong> 高斯误差线性单元(Gaussian Error Linear Uints)，激活函数</p>
</blockquote>
]]></content>
      <categories>
        <category>PaperNotes</category>
      </categories>
      <tags>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>TransGAN</title>
    <url>/2021/12/01/TransGAN%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<blockquote>
<p>代码来源 <a href="https://github.com/asarigun/TransGAN">https://github.com/asarigun/TransGAN</a></p>
</blockquote>
<h1 id="TransGAN-Two-Transformers-Can-Make-One-Strong-GAN"><a href="#TransGAN-Two-Transformers-Can-Make-One-Strong-GAN" class="headerlink" title="TransGAN: Two Transformers Can Make One Strong GAN"></a>TransGAN: Two Transformers Can Make One Strong GAN</h1><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="/2021/12/01/TransGAN%E7%AC%94%E8%AE%B0/TransGAN.png" alt="transgan"><br><span id="more"></span></p>
<h2 id="Generator部分"><a href="#Generator部分" class="headerlink" title="Generator部分"></a>Generator部分</h2><h3 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_feat, hid_feat=<span class="literal">None</span>, out_feat=<span class="literal">None</span>, dropout=<span class="number">0.</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> hid_feat:</span><br><span class="line">            hid_feat = in_feat</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> out_feat:</span><br><span class="line">            out_feat = in_feat</span><br><span class="line">        self.fc1 = nn.Linear(in_fat, hid_feat)</span><br><span class="line">        self.act = nn.GELU()</span><br><span class="line">        self.fc2 = nn.Linear(hid_feat, out_feat)</span><br><span class="line">        self.droprateout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.act(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> self.droprateout(x)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><a href="https://arxiv.org/pdf/1606.08415.pdf">GELU(Gussian Error Linear Units, 高斯线性误差线性单元)激活函数</a></p>
<script type="math/tex; mode=display">xP(X\leq x) = x \Phi(x)=x\int_{-\infty}^x\frac{e^{-\frac{(X-\mu)^2}{2\sigma^2}}}{\sqrt{2\pi}\sigma} \rm d x</script><p>$\Phi(x)$指的是$x$的高斯正态分布的累计分布</p>
</blockquote>
<p>并且在每次输入TranformerEncoder之前加上一个可学习的<strong>位置embedding</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.positional_embedding_1 = nn.Parameter(torch.zeros(<span class="number">1</span>, (<span class="number">8</span> ** <span class="number">2</span>), <span class="number">384</span>)， requires_grad=<span class="literal">True</span>)  <span class="comment"># 位置embedding</span></span><br><span class="line">x =  x + self.positional_embedding_1</span><br></pre></td></tr></table></figure></p>
<h3 id="Transformer-Encoder"><a href="#Transformer-Encoder" class="headerlink" title="Transformer Encoder"></a>Transformer Encoder</h3><p>Transformer Encoder通过堆叠Encoder_Block组成，TransGAN使用了tranformer中一样的transformer encoder模块。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder_Block</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, heads, mlp_ratio=<span class="number">4</span>, drop_rate=<span class="number">0.</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.ln1 = nn.LayerNorm(dim)</span><br><span class="line">        self.attn = Attention(dim, heads, drop_rate, drop_rate)</span><br><span class="line">        self.ln2 = nn.LayerNorm(dim)</span><br><span class="line">        self.mlp = MLP(dim, dim * mlp_ratio, dropout=drop_rate)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x1 = self.ln1(x)</span><br><span class="line">        x = x + self.attn(x1)</span><br><span class="line">        x2 = self.ln2(x)</span><br><span class="line">        x = x + self.mlp(x2)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p>
<h3 id="Encoder-模块中使用的attention机制："><a href="#Encoder-模块中使用的attention机制：" class="headerlink" title="Encoder 模块中使用的attention机制："></a>Encoder 模块中使用的attention机制：</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, heads=<span class="number">4</span>, attention_dropout=<span class="number">0.</span>, proj_dropout=<span class="number">0.</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.heads = heads</span><br><span class="line">        self.scale = <span class="number">1.</span> / dim ** <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        self.qkv = nn.Linear(dim, dim * <span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.attention_dropout = nn.Dropout(attention_dropout)</span><br><span class="line">        self.out = nn.Sequential(</span><br><span class="line">            nn.Linear(dim, dim),</span><br><span class="line">            nn.Dropout(proj_dropout)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        b, n, c = x.shape</span><br><span class="line">        qkv = self.qkv(x).reshape(b, n, <span class="number">3</span>, self.heads, c // self.heads)</span><br><span class="line">        q, k, v = qkv.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        dot = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) * self.scale</span><br><span class="line">        attn = dot.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        attn = self.attention_dropout(attn)</span><br><span class="line"></span><br><span class="line">        x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(b, n, c)</span><br><span class="line">        x = self.out(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="TransGAN使用PixelShuffle进行上采样"><a href="#TransGAN使用PixelShuffle进行上采样" class="headerlink" title="TransGAN使用PixelShuffle进行上采样"></a>TransGAN使用<a href="https://arxiv.org/abs/1609.05158">PixelShuffle</a>进行上采样</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">UpSampling</span>(<span class="params">x, H, W</span>):</span></span><br><span class="line">    B, N, C = x.size()</span><br><span class="line">    <span class="keyword">assert</span> N == H * W</span><br><span class="line">    x = x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    x = x.view(-<span class="number">1</span>, C, H, W)</span><br><span class="line">    x = nn.PixelShuffle(<span class="number">2</span>)(x)</span><br><span class="line">    B, C, H, W = x.size()</span><br><span class="line">    x = x.view(-<span class="number">1</span>, C, H * W)</span><br><span class="line">    x = x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> x, H, W</span><br></pre></td></tr></table></figure>
<h3 id="LinearFlattern部分将输入转换成输出图像"><a href="#LinearFlattern部分将输入转换成输出图像" class="headerlink" title="LinearFlattern部分将输入转换成输出图像"></a>LinearFlattern部分将输入转换成输出图像</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = self.linear(x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>).view(-<span class="number">1</span>, self.dim // <span class="number">16</span>, H, W))</span><br></pre></td></tr></table></figure>
<h3 id="Generator的定义"><a href="#Generator的定义" class="headerlink" title="Generator的定义"></a>Generator的定义</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;docstring for Generator&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, depth1=<span class="number">5</span>, depth2=<span class="number">4</span>, depth3=<span class="number">2</span>, initial_size=<span class="number">8</span>, dim=<span class="number">384</span>, heads=<span class="number">4</span>, mlp_ratio=<span class="number">4</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 drop_rate=<span class="number">0.</span></span>):</span>  <span class="comment"># ,device=device):</span></span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.device = device</span></span><br><span class="line">        self.initial_size = initial_size</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.depth1 = depth1</span><br><span class="line">        self.depth2 = depth2</span><br><span class="line">        self.depth3 = depth3</span><br><span class="line">        self.heads = heads</span><br><span class="line">        self.mlp_ratio = mlp_ratio</span><br><span class="line">        self.drop_rate = drop_rate</span><br><span class="line"></span><br><span class="line">        self.mlp = nn.Linear(<span class="number">1024</span>, (self.initial_size ** <span class="number">2</span>) * self.dim)</span><br><span class="line"></span><br><span class="line">        self.positional_embedding_1 = nn.Parameter(torch.zeros(<span class="number">1</span>, (<span class="number">8</span> ** <span class="number">2</span>), <span class="number">384</span>))</span><br><span class="line">        self.positional_embedding_2 = nn.Parameter(torch.zeros(<span class="number">1</span>, (<span class="number">8</span> * <span class="number">2</span>) ** <span class="number">2</span>, <span class="number">384</span> // <span class="number">4</span>))</span><br><span class="line">        self.positional_embedding_3 = nn.Parameter(torch.zeros(<span class="number">1</span>, (<span class="number">8</span> * <span class="number">4</span>) ** <span class="number">2</span>, <span class="number">384</span> // <span class="number">16</span>))</span><br><span class="line"></span><br><span class="line">        self.TransformerEncoder_encoder1 = TransformerEncoder(depth=self.depth1, dim=self.dim, heads=self.heads,</span><br><span class="line">                                                              mlp_ratio=self.mlp_ratio, drop_rate=self.drop_rate)</span><br><span class="line">        self.TransformerEncoder_encoder2 = TransformerEncoder(depth=self.depth2, dim=self.dim // <span class="number">4</span>, heads=self.heads,</span><br><span class="line">                                                              mlp_ratio=self.mlp_ratio, drop_rate=self.drop_rate)</span><br><span class="line">        self.TransformerEncoder_encoder3 = TransformerEncoder(depth=self.depth3, dim=self.dim // <span class="number">16</span>, heads=self.heads,</span><br><span class="line">                                                              mlp_ratio=self.mlp_ratio, drop_rate=self.drop_rate)</span><br><span class="line"></span><br><span class="line">        self.linear = nn.Sequential(nn.Conv2d(self.dim // <span class="number">16</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, noise</span>):</span></span><br><span class="line">        x = self.mlp(noise).view(-<span class="number">1</span>, self.initial_size ** <span class="number">2</span>, self.dim)</span><br><span class="line"></span><br><span class="line">        x = x + self.positional_embedding_1</span><br><span class="line">        H, W = self.initial_size, self.initial_size</span><br><span class="line">        x = self.TransformerEncoder_encoder1(x)</span><br><span class="line"></span><br><span class="line">        x, H, W = UpSampling(x, H, W)</span><br><span class="line">        x = x + self.positional_embedding_2</span><br><span class="line">        x = self.TransformerEncoder_encoder2(x)</span><br><span class="line"></span><br><span class="line">        x, H, W = UpSampling(x, H, W)</span><br><span class="line">        x = x + self.positional_embedding_3</span><br><span class="line"></span><br><span class="line">        x = self.TransformerEncoder_encoder3(x)</span><br><span class="line">        x = self.linear(x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>).view(-<span class="number">1</span>, self.dim // <span class="number">16</span>, H, W))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="Discriminator部分"><a href="#Discriminator部分" class="headerlink" title="Discriminator部分"></a>Discriminator部分</h2><p>图像在输入Discriminator之前需要先分成几个patch。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImgPatches</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_channel=<span class="number">3</span>, dim=<span class="number">768</span>, patch_size=<span class="number">4</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.patch_embed = nn.Conv2d(input_channel, dim,</span><br><span class="line">                                     kernel_size=patch_size, stride=patch_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, img</span>):</span></span><br><span class="line">        patches = self.patch_embed(img).flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> patches</span><br></pre></td></tr></table></figure><br>并且加上一个<strong>class token</strong>作为最后判别器的分类结果。<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.class_embedding = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, dim))</span><br></pre></td></tr></table></figure><br>为了保留位置信息，图片块在输入判别器时加上<strong>位置embedding</strong>。<br>判别器的TransformerEncoder结构与生成器基本一致。<br>所以Discriminator的代码部分如下：<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, diff_aug, image_size=<span class="number">256</span>, patch_size=<span class="number">8</span>, input_channel=<span class="number">3</span>, num_classes=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dim=<span class="number">384</span>, depth=<span class="number">7</span>, heads=<span class="number">4</span>, mlp_ratio=<span class="number">4</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 drop_rate=<span class="number">0.</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> image_size % patch_size != <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Image size must be divisible by patch size.&#x27;</span>)</span><br><span class="line">        num_patches = (image_size // patch_size) ** <span class="number">2</span></span><br><span class="line">        self.diff_aug = diff_aug</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.depth = depth</span><br><span class="line">        <span class="comment"># Image patches and embedding layer</span></span><br><span class="line">        self.patches = ImgPatches(input_channel, dim, self.patch_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Embedding for patch position and class</span></span><br><span class="line">        self.positional_embedding = nn.Parameter(torch.zeros(<span class="number">1</span>, num_patches + <span class="number">1</span>, dim))</span><br><span class="line">        self.class_embedding = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, dim))</span><br><span class="line">        nn.init.trunc_normal_(self.positional_embedding, std=<span class="number">0.2</span>)</span><br><span class="line">        nn.init.trunc_normal_(self.class_embedding, std=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">        self.droprate = nn.Dropout(p=drop_rate)</span><br><span class="line">        self.TransfomerEncoder = TransformerEncoder(depth, dim, heads,</span><br><span class="line">                                                    mlp_ratio, drop_rate)</span><br><span class="line">        self.norm = nn.LayerNorm(dim)</span><br><span class="line">        self.out = nn.Linear(dim, num_classes)</span><br><span class="line">        self.apply(self._init_weights)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_weights</span>(<span class="params">self, m</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">            nn.init.trunc_normal_(m.weight, std=<span class="number">.02</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Linear) <span class="keyword">and</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.LayerNorm):</span><br><span class="line">            nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">            nn.init.constant_(m.weight, <span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = DiffAugment(x, self.diff_aug)</span><br><span class="line">        b = x.shape[<span class="number">0</span>]</span><br><span class="line">        cls_token = self.class_embedding.expand(b, -<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        x = self.patches(x)</span><br><span class="line">        x = torch.cat((cls_token, x), dim=<span class="number">1</span>)</span><br><span class="line">        x += self.positional_embedding</span><br><span class="line">        x = self.droprate(x)</span><br><span class="line">        x = self.TransfomerEncoder(x)</span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        x = self.out(x[:, <span class="number">0</span>])</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>PaperNotes</category>
      </categories>
      <tags>
        <tag>transformer</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>ViT</title>
    <url>/2021/12/10/ViT%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="An-Image-is-Worth-16x16-Words-Transformer-for-Image-Recognition-at-Scale-Google-Brain"><a href="#An-Image-is-Worth-16x16-Words-Transformer-for-Image-Recognition-at-Scale-Google-Brain" class="headerlink" title="An Image is Worth 16x16 Words: Transformer for Image Recognition at Scale(Google Brain)"></a><a href="https://github.com/google-research/vision_transformer">An Image is Worth 16x16 Words: Transformer for Image Recognition at Scale(Google Brain)</a></h2><blockquote>
<p>Vision Transformer (ViT)</p>
</blockquote>
<h3 id="Model-Overview"><a href="#Model-Overview" class="headerlink" title="Model Overview"></a>Model Overview</h3><p><img src="/2021/12/10/ViT%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/ModelOverview.png" alt="ModelOverview"></p>
<p>ViT将图像裁剪成固定尺寸的图像块，对图像块做线性的embedding，并且加入位置embedding，为了用于分类任务，对输入的图像块序列还加入了可学习的classification token。<br><span id="more"></span></p>
<blockquote>
<p>TODO： image embedding</p>
</blockquote>
<h3 id="ViT"><a href="#ViT" class="headerlink" title="ViT"></a>ViT</h3><p>用下列公式表示ViT的工作机制</p>
<script type="math/tex; mode=display">z_0 = [x_{class}; x_p^1 \mathbf{E}; x_p^2\mathbf{E}; \ldots ; x_p^N \mathbf{E}] + \mathbf{E}_{pos};  \qquad \mathbf{E} \in \mathbb{R}^{(P^2\cdot C)\cdot D}, \mathbf{E}_{pos} \in \mathbb{R}^{(N+1)\cdot D}</script><blockquote>
<p>patch embedding操作, $z_0^0=x_{class}$为可学习的embedding, $\mathbf{E}_{pos}$为位置embedding。</p>
</blockquote>
<script type="math/tex; mode=display">z^{\prime}_{\mathcal{l}} = MSA(LN(z_{\mathcal{l}-1}) + z_{\mathcal{l}-1}); \qquad \mathcal{l} = 1, \ldots, L</script><blockquote>
<p>Multiheaded self-attention(MSA); LayerNorm(LN)</p>
</blockquote>
<script type="math/tex; mode=display">z_{\mathcal{l}} = MLP(LN(z^{\prime})) + z^{\prime}_{\mathcal{l}}; \qquad \mathcal{l} = 1, \ldots , L</script><script type="math/tex; mode=display">y = LN(z^0_L)</script><p>ViT 的输入为1D的token embedding 序列，为了处理2D图像，现将图像reshape处理，$(P, P)$是每个图像块的分辨率，$N = HW /P^2$是产生的图像块的数量，也是transformer 的输入序列的长度， $C$是图像的通道数:</p>
<script type="math/tex; mode=display">x \in \mathbb{R}^{H * W * C} \rightarrow x\in \mathbb{R}^{N * (P^2 \cdot C)}</script><p>由于transformer 在其所有层中使用恒定大小$D$的潜在向量，所有ViT将图像块展开，并使用可训练的线性投影映射将图像块投影到$D$维空间中，这个映射的结果即为patch embedding。<br>同时，transformer的输入序列加入了positon embedding 保留位置信息。加入一个可学习的类别embedding，其在tranformer encoder 的输出状态被认为是图像表征$y$。<br>classification head 由一个MLP和一层隐藏层（预训练时）/线性层（微调时）组成。<br>transformer encoder 的MLP包含两个具有GELU非线性的层。  </p>
]]></content>
      <categories>
        <category>PaperNotes</category>
      </categories>
      <tags>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>anaconda使用问题汇总</title>
    <url>/2021/11/29/anaconda%E5%A1%AB%E5%9D%91/</url>
    <content><![CDATA[<h3 id="conda-安装库时出现proxy问题"><a href="#conda-安装库时出现proxy问题" class="headerlink" title="conda 安装库时出现proxy问题"></a>conda 安装库时出现proxy问题</h3><h4 id="方法一：关闭本机网络代理"><a href="#方法一：关闭本机网络代理" class="headerlink" title="方法一：关闭本机网络代理"></a>方法一：关闭本机网络代理</h4><h4 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h4><p>修改~目录下的<code>.condarc</code>文件，添加或者修改<code>proxy_server</code>参数，添加格式如下<span id="more"></span><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">proxy_servers:</span><br><span class="line">    http: http://127.0.0.1:8888</span><br><span class="line">    https: http://127.0.0.1:8888</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>! https: http 而不是https</p>
<p>配置socks5代理：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">proxy_servers:</span><br><span class="line">  http: socks5://127.0.0.1:1089</span><br><span class="line">  https: socks5://127.0.0.1:1089</span><br></pre></td></tr></table></figure><br>注意不要用<code>tab</code>键，而是使用四个空格，<code>https</code>和<code>http</code>后有一空格。代理ip地址为本机的<code>http</code>和<code>https</code>代理ip及端口，而不是远程服务器的ip和端口。</p>
</blockquote>
<h4 id="方法三"><a href="#方法三" class="headerlink" title="方法三"></a>方法三</h4><p><code>env  | grep -i &quot;_PROXY&quot;</code> 查看环境变量<br><code>unset http_proxy &amp;&amp; https_proxy &amp;&amp; HTTP_PROXY &amp;&amp; HTTPS_PROXY</code>  并且<code>export all_proxy=&quot;socks://127.0.01.:1088&quot;</code>，注释<code>.condarc</code>文件中的proxy_server设置,即可。<br>未关闭本机网络代理。</p>
<blockquote>
<p>不确定<code>unset...</code>和<code>export...</code>两个命令之间是否存在关联 </p>
</blockquote>
]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>anaconda</tag>
        <tag>conda</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention is All You Need</title>
    <url>/2021/12/29/attention_is_all_you_need/</url>
    <content><![CDATA[<h2 id="Attention-is-All-You-Need-google-brain"><a href="#Attention-is-All-You-Need-google-brain" class="headerlink" title="Attention is All You Need(google brain)"></a>Attention is All You Need(google brain)</h2><ul>
<li>完全使用注意力机制学习输入输出之间的全局依赖（训练时允许并行计算，规避顺序计算的基本约束）<span id="more"></span>
<h3 id="transformer的模型结构"><a href="#transformer的模型结构" class="headerlink" title="transformer的模型结构"></a>transformer的模型结构</h3><strong>总体结构如下</strong><br><img src="/2021/12/29/attention_is_all_you_need/transformer.png" alt="transformer"><br>transformer 包括<strong>Encoder</strong>和<strong>Decoder</strong>两大块。<br><strong>Encoder</strong>部分由$N=6$个相同的层组成，每块有两个子层；两个子层分别为<em>multi-head self-attention mechanism</em>和<em>fully connected feed-forward network</em>；并且两个子层都使用了<em>residual connection</em>和<em>layer normalization</em>，所以每个子层的输出为$LayerNorm(x+SubLayer(x))$。<br><strong>Decoder</strong>部分和<strong>Encoder</strong>相似，也有6个相同的层组成。不同的是，<strong>Decoder</strong>在每层中插入了一个新的子层<em>Multi-Head Attention</em>用于对<strong>Encoder</strong>的输出执行<em>Multi-Head Attention</em>。<blockquote>
<p>修改了decoder堆栈中的自注意力子层，以防止位置关注后续位置，确保位置i的预测只能依赖与小于i位置的已知输出?</p>
</blockquote>
</li>
</ul>
<h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><h4 id="transformer-提出了一种特殊的注意力机制-Scaled-Dot-Product-Attention"><a href="#transformer-提出了一种特殊的注意力机制-Scaled-Dot-Product-Attention" class="headerlink" title="transformer 提出了一种特殊的注意力机制 Scaled Dot-Product Attention"></a>transformer 提出了一种特殊的注意力机制 Scaled Dot-Product Attention</h4><blockquote>
<p>两种常用的注意力函数为<em>additive attention</em>和<em>dot-product(multi-plicative) attention</em>; </p>
</blockquote>
<p><em>scaled dot-product attention</em>只比<em>dot-product attention</em>多了个缩放因子$\frac{1}{\sqrt{d_k}}$;添加$\frac{1}{\sqrt{d_k}}$是由于作者怀疑在$k$较大时，<em>dot-product</em>的量级变大，会将<em>softmax</em>函数推向梯度较小的区间</p>
<p><img src="/2021/12/29/attention_is_all_you_need/ScaledDot-ProductAttention.png" alt="attention"><br><strong>Scaled Dot-product Attention</strong>的输入包括$d_k$维的$keys$, $d_v$维的$values$和$queries$。通过$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$得到点积结果</p>
<h4 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h4><p>对<em>q</em>, <em>k</em>, <em>v</em>使用不同的可学习的线性projection；</p>
<h4 id="Transformer-以三种方式使用multi-head-attention"><a href="#Transformer-以三种方式使用multi-head-attention" class="headerlink" title="Transformer 以三种方式使用multi-head attention"></a>Transformer 以三种方式使用<em>multi-head attention</em></h4><ul>
<li><em>encoder-decoder attention</em>层中使用<em>multi-head attention</em>，<em>query</em>来自前一个解码器，<em>memeory key</em>和<em>value</em>来自编码起的输出；</li>
<li><em>encoder</em>中的<em>self-attention layers</em>，所有的<em>query</em>, <em>key</em>和<em>value</em>都来自上一个编码器，使得编码器中的每个位置都可以参与到前一层编码器的所有位置；</li>
<li><em>decoder</em>中<em>self-attention layers</em>的使用与<em>encoder</em>中相似<blockquote>
<p>需要放置解码器中的左向信息流以保留自回归特性</p>
</blockquote>
</li>
</ul>
<h3 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h3><p><em>transformer</em>在网络中使用了多个相同结构的全连接<em>feed-forward network</em>。<em>feed-forwardnetwork</em>包含两个线性层和<em>ReLU</em>激活函数。</p>
<script type="math/tex; mode=display">FFN(x) = \max (0, xW_1 + b_1)W_2 + b_2</script><h3 id="Embedding-and-Softmax"><a href="#Embedding-and-Softmax" class="headerlink" title="Embedding and Softmax"></a>Embedding and Softmax</h3><p>使用<em>embeddings</em>将输入<em>token</em>和输出<em>token</em>转换为$d_{model}$维的向量；</p>
<h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>由于<em>transformer</em>不包含循环或卷积结构，为了利用输入序列的位置信息，在编码器和解码器堆栈的底部对输入的<em>emdeddings</em>使用了<em>positional encoding</em>，并且<em>transformer</em>使用了<em>sine and cosine functions</em>:</p>
<script type="math/tex; mode=display">PE_{(pod, 2i)} = \sin (pos / 10000^{(2i / d_{model}}))</script><script type="math/tex; mode=display">PE_{(pod, 2i)+1} = \cos (pos / 10000^{(2i / d_{model}}))</script><p>$pos$代表位置，$i$代表维度</p>
]]></content>
      <categories>
        <category>PaperNotes</category>
      </categories>
      <tags>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>git 创建分支并上传文件</title>
    <url>/2022/01/02/git%E5%88%9B%E5%BB%BA%E5%88%86%E6%94%AF%E5%B9%B6%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6/</url>
    <content><![CDATA[<pre><code>git clone http://github.com/qqq27/Project.git
git branch lq  # 创建一个名为lq的新分支
git checkout branch lq
cd folder
git add .
git commit -m &quot;  &quot;
git push origin lq
git branch -d lq ## 本地删除分支
git push origin :lq ## 删除远程分支
</code></pre>]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>blog搭建过程备忘</title>
    <url>/2021/11/27/hexo+github%E6%90%AD%E5%BB%BAblog/</url>
    <content><![CDATA[<p>本地环境配置参考了<a href="https://segmentfault.com/a/1190000017986794">blog</a></p>
<h2 id="本地环境配置"><a href="#本地环境配置" class="headerlink" title="本地环境配置"></a>本地环境配置</h2><h3 id="NodeJS"><a href="#NodeJS" class="headerlink" title="NodeJS"></a>NodeJS</h3><blockquote>
<p>Hexo 基于Node.js驱动</p>
</blockquote>
<h3 id="git"><a href="#git" class="headerlink" title="git"></a>git</h3><span id="more"></span>
<h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git version</span><br><span class="line">node -v</span><br><span class="line">npm -v</span><br></pre></td></tr></table></figure>
<h3 id="Hexo安装及配置"><a href="#Hexo安装及配置" class="headerlink" title="Hexo安装及配置"></a>Hexo安装及配置</h3><p><code>npm install -g hexo-cli</code>  </p>
<h4 id="如果安装时发生代理错误：FetchError"><a href="#如果安装时发生代理错误：FetchError" class="headerlink" title="如果安装时发生代理错误：FetchError"></a>如果安装时发生代理错误：FetchError</h4><p>换源：<code>npm config set registry https://registry.npm.taobao.org</code><br>清除代理： <code>npm config set proxy null</code>; <code>npm config set https-proxy null</code></p>
<p>安装完成后，执行下列命令，在指定文件夹中新建文件；<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo init myblog</span><br><span class="line">cd myblog</span><br><span class="line">npm install </span><br></pre></td></tr></table></figure><br>生成的文件夹目录如下；<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── _config.yml # 网站的配置信息，您可以在此配置大部分的参数。 </span><br><span class="line">├── package.json</span><br><span class="line">├── scaffolds # 模版文件夹</span><br><span class="line">├── source  # 资源文件夹，除 _posts 文件，其他以下划线_开头的文件或者文件夹不会被编译打包到public文件夹</span><br><span class="line">|   ├── _drafts # 草稿文件</span><br><span class="line">|   └── _posts # 文章Markdowm文件 </span><br><span class="line">└── themes  # 主题文件夹</span><br></pre></td></tr></table></figure><br>生成完成后，运行<code>hexo s</code>，在浏览器打开<a href="http://localhost:4000">http://localhost:4000</a> 即可预览效果</p>
<h2 id="部署到Github"><a href="#部署到Github" class="headerlink" title="部署到Github"></a>部署到Github</h2><h3 id="更改-cong-yml-配置文件配置参数"><a href="#更改-cong-yml-配置文件配置参数" class="headerlink" title="更改_cong.yml 配置文件配置参数"></a>更改_cong.yml 配置文件配置参数</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">    type: git </span><br><span class="line">    repo: </span><br><span class="line">        github: https://github.com/QQQ27/QQQ27.github.io.git</span><br><span class="line">    branch: master</span><br></pre></td></tr></table></figure>
<h3 id="安装部署插件"><a href="#安装部署插件" class="headerlink" title="安装部署插件"></a>安装部署插件</h3><p><code>npm install hexo-deployer-git --save</code></p>
<h3 id="部署上传"><a href="#部署上传" class="headerlink" title="部署上传"></a>部署上传</h3><p><code>hexo g -d</code><br>生成上传完成后，访问<a href="https://QQQ27.github.io">https://QQQ27.github.io</a> 即可看到搭建的博客主页。</p>
<h2 id="更改博客主题"><a href="#更改博客主题" class="headerlink" title="更改博客主题"></a>更改博客主题</h2><p>以next主题为例，克隆next项目到hexo根目录的theme/next文件中，修改hexo根目录的_config.yml配置文件中的theme为next即可。修改完成后，可以使用<code>`hexo s --debug</code>命令验证。</p>
<h3 id="debug没有出错，但是博客页面无法正常显示？"><a href="#debug没有出错，但是博客页面无法正常显示？" class="headerlink" title="debug没有出错，但是博客页面无法正常显示？"></a>debug没有出错，但是博客页面无法正常显示？</h3><p>由于hexo在5.0版本后删除了swig，需要手动安装：<code>npm i hexo-render-swig</code>。</p>
<p><a href="http://theme-next.iissnan.com/">next 主页</a><br><a href="https://github.com/iissnan/hexo-theme-next">next github主页</a><br><a href="https://www.techgrow.cn/posts/755ff30d.html">参考blog</a></p>
]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>博客搭建</tag>
      </tags>
  </entry>
  <entry>
    <title>ubuntu 迁移conda虚拟环境</title>
    <url>/2021/10/04/linux%20%E8%BF%81%E7%A7%BBanaconda%20%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/</url>
    <content><![CDATA[<blockquote>
<p>两台计算机均为ubuntu系统，远程服务器无法使用http协议<br> 两台不同操作系统的环境迁移使用yaml文件</p>
</blockquote>
<h3 id="conda-clone"><a href="#conda-clone" class="headerlink" title="conda clone"></a>conda clone</h3><ul>
<li>本地计算机，直接clone本地文件夹 /anaconda3/pkgs 和/anaconda3/envs/ my_env至远程服务器的相应文件夹；<span id="more"></span></li>
<li>远程服务器上，<code>conda create -n my_env --clone  my_env_path --offline</code> 发生HTTPERROR</li>
</ul>
<blockquote>
<p>指定 <code>-offline</code>参数后，安装依然发生HTTP ERROR，为conda 版本bug</p>
</blockquote>
<h3 id="conda-pack-and-unpack-有效"><a href="#conda-pack-and-unpack-有效" class="headerlink" title="conda pack and unpack(有效)"></a>conda pack and unpack(有效)</h3><ul>
<li>本地计算机上，在base环境下（保证各个子环境都可以使用）<code>conda install  - c conda-forge conda-pack</code>或者<code>pip install conda-pack</code></li>
<li><code>conda activate my_env</code>，打包本地虚拟环境 <code>conda-pack</code> 得到 my_env.tar.gz（压缩包在当前命令行目录下），将 my_env.tar.gz传输到远程服务器</li>
<li>在远程服务器上解压，<code>tar -xf my_env.tar.gz -C ~/anaconda3/envs/my_env</code>，激活虚拟环境 <code>source ~/anaconda3/envs/my_env/bin/activate</code></li>
</ul>
<blockquote>
<p>解压过程出现<code>tar my_env.tar.gz ~/anaconda3/envs/my_env</code> 出现<br>tar: xx: Not Found in archive ; tar: Exiting with failure status due to previous errors 错误<br>原因是因为压缩文件使用的相对路径，在当前目录下找不到/usr目录，通过使用<code>-C</code>指定解压目录可解决此问题<br><code>tar -zxvf my_env.tar.gz -C ~/anaconda3/envs/my_env</code></p>
</blockquote>
]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>conda</tag>
      </tags>
  </entry>
  <entry>
    <title>VS Code 编程环境配置汇总</title>
    <url>/2021/09/16/vs%20code%E9%85%8D%E7%BD%AE%E5%90%84%E7%A7%8D%E8%AF%AD%E8%A8%80%E7%8E%AF%E5%A2%83%E6%B1%87%E9%9B%86/</url>
    <content><![CDATA[<blockquote>
<p>第一次打开vs code，首先安装chinese(/doge);</p>
<h2 id="MarkDown-环境配置"><a href="#MarkDown-环境配置" class="headerlink" title="MarkDown 环境配置"></a>MarkDown 环境配置</h2><p>Markdown的配置十分简单，安装Markdown All in One 即可，为了更好的体验感，推荐安装Markdown Preview Github 和 Markdown Preview Enhanced。<br><span id="more"></span></p>
<h2 id="Latex-环境配置"><a href="#Latex-环境配置" class="headerlink" title="Latex 环境配置"></a>Latex 环境配置</h2><h3 id="实现tex文件和pdf的正向和反向搜索"><a href="#实现tex文件和pdf的正向和反向搜索" class="headerlink" title="实现tex文件和pdf的正向和反向搜索"></a>实现tex文件和pdf的正向和反向搜索</h3><p><code>ctl+alt+j</code>打开外部浏览器<br>主要参考<a href="https://zhuanlan.zhihu.com/p/166523064">参考1</a>配置settings.json文件，同时参照<a href="https://blog.csdn.net/qq_24502469/article/details/114269806?spm=1001.2101.3001.6650.2&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-2.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-2.no_search_link">参考2</a>修改json文件中正向搜索的配置。<br><a href="https://blog.csdn.net/qq_31831923/article/details/105823101">参考3</a><br>反向搜索中注意修改sumatraPDF的设置</p>
</blockquote>
<ul>
<li>正向搜索，固定源码中的光标<code>ctl+alt+j</code></li>
<li>反向搜索，<code>ctl</code>+鼠标左键</li>
</ul>
<h2 id="Python-环境配置"><a href="#Python-环境配置" class="headerlink" title="Python 环境配置"></a>Python 环境配置</h2><p>简单，如果安装了anaconda直接选择anaconda的python就好了</p>
<h2 id="C-C环境配置"><a href="#C-C环境配置" class="headerlink" title="C++/C环境配置"></a>C++/C环境配置</h2><h3 id="配置编译器-MinGW-w64"><a href="#配置编译器-MinGW-w64" class="headerlink" title="配置编译器 MinGW-w64"></a>配置编译器 MinGW-w64</h3><p>有很多种下载方式，这里提供<a href="https://sourceforge.net/projects/mingw-w64/files/mingw-w64/">Online Installer</a>下载地址。</p>
<p>installer安装时的选择说明如下<img src="https://pic3.zhimg.com/80/v2-11fc41cf59662dfd55bfcadcbc6fe9fa_1440w.jpg" alt></p>
<blockquote>
<p>安装路径不能含空格，需要默认安装地址c:/Program Files/…</p>
</blockquote>
<h4 id="安装后添加环境变量"><a href="#安装后添加环境变量" class="headerlink" title="安装后添加环境变量"></a>安装后添加环境变量</h4><p>将mingw64下的bin文件夹添加至path;</p>
<h4 id="检查"><a href="#检查" class="headerlink" title="检查"></a>检查</h4><p><code>g++ --version</code><br><code>gdb --version</code></p>
<h3 id="设置各种-json文件"><a href="#设置各种-json文件" class="headerlink" title="设置各种.json文件"></a>设置各种.json文件</h3><blockquote>
<p>vs code 终端输出中文为乱码问题：右下角更改编码保存方式<code>通过编码保存``$$\rightarrow$$``GBK</code>格式</p>
<p><code>Run Code</code>输出错误，<code>gcc/g++ .......</code>：检查mingw的bin文件夹是否加入环境变量，若已加入；重启vs code。</p>
</blockquote>
<p>参照[<a href="https://zhuanlan.zhihu.com/p/105135431][https://zhuanlan.zhihu.com/p/87864677">https://zhuanlan.zhihu.com/p/105135431][https://zhuanlan.zhihu.com/p/87864677</a>]</p>
]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>VS Code</tag>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title>微软商店无法打开</title>
    <url>/2022/01/30/%E5%BE%AE%E8%BD%AF%E5%95%86%E5%BA%97%E7%BD%91%E7%BB%9C%E5%8E%9F%E5%9B%A0%E6%97%A0%E6%B3%95%E6%89%93%E5%BC%80/</url>
    <content><![CDATA[<p>microsoft store网络原因无法打开<br>开始菜单—&gt;网络和Internet—&gt;代理—&gt;开启自动检测设置，不使用设置脚本</p>
]]></content>
      <categories>
        <category>网络代理</category>
      </categories>
      <tags>
        <tag>微软商店</tag>
      </tags>
  </entry>
  <entry>
    <title>代理服务器维护</title>
    <url>/2021/07/14/%E6%9C%8D%E5%8A%A1%E5%99%A8ip%E6%88%96port%E8%A2%AB%E5%B0%81%E7%BB%B4%E6%8A%A4/</url>
    <content><![CDATA[<h2 id="梯子又挂了！！！"><a href="#梯子又挂了！！！" class="headerlink" title="梯子又挂了！！！"></a>梯子又挂了！！！</h2><p>租用vps搭建的v2ray用于科学上网，但是由于zz原因，每隔一两个月都会发生ip被封或者端口被封。</p>
<p>可以使用<a href="http://port.ping.pe/">ip检测网站</a>判断是否使服务器的问题或者判断到底是ip被封还是port被封。  </p>
<h3 id="port被封"><a href="#port被封" class="headerlink" title="port被封"></a>port被封</h3><p>如果只是端口被封，只需要重新开启一个端口，并且修改v2ray的配置文件（/etc/v2ray/config.json）即可。  <span id="more"></span><br><code>ufw status</code> 查看已开启端口<br><code>ufw allow 123456</code> 打开端口<br><code>ufw enable</code>  开启防火墙<br><code>ufw reload</code>  重启防火墙<br><code>ufw status</code>  查看端口是否开放  </p>
<blockquote>
<p>修改端口后建议重启服务器</p>
</blockquote>
<p>一般来说做到这步之后再更改配置文件就可以了，但是也可能出现端口已开放，但是还是通过端口无法访问的情况。这种情况可能是由于开启的端口上没有程序监听导致关闭。<br><code>netstat -ntlp | grep LISTEN</code> 查看打开了哪些端口，如果没有新开启的端口，则<br><code>nohup python3 -m http.server 10669 &amp;</code>  低于python3之前的python版本应该使用<code>nohup python -m SimpleHTTPServer 123456 &amp;</code></p>
<blockquote>
<p>在命令后面加上<code>&amp;</code>使命令产生的进程在后台运行，不影响当前终端的使用（否则命令会把进程产生的信息输出在终端）；<br>命令前加<code>nohup</code> 使得即使关闭当前终端，也不会kill 该命令产生的进程；</p>
</blockquote>
<h3 id="ip被封！！！"><a href="#ip被封！！！" class="headerlink" title="ip被封！！！"></a>ip被封！！！</h3><blockquote>
<p>体验过自建shadowsocks 和 v2ray， v2ray的封ip概率很低，一般都是被封端口，容易维护。</p>
<p>如果是ip被封，那么非常不幸的，只能重新开启一个新Ip了。</p>
</blockquote>
]]></content>
      <categories>
        <category>网络代理</category>
      </categories>
      <tags>
        <tag>网络代理</tag>
      </tags>
  </entry>
  <entry>
    <title>统计机器学习课程笔记</title>
    <url>/2021/11/10/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%AF%B9%E6%8A%97%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="Bayesian-Decision-Theory-and-Gaussina-Pttenr-Classifiers"><a href="#Bayesian-Decision-Theory-and-Gaussina-Pttenr-Classifiers" class="headerlink" title="Bayesian Decision Theory and Gaussina Pttenr  Classifiers"></a>Bayesian Decision Theory and Gaussina Pttenr  Classifiers</h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul>
<li>贝叶斯决策理论是统计机器学习的基础</li>
<li>贝叶斯理论假设概率已知或可获得</li>
<li>贝叶斯决策理论兼顾决策概率和决策风险<span id="more"></span>
<h3 id="the-MAP-maximum-a-posteriori-decision-rule"><a href="#the-MAP-maximum-a-posteriori-decision-rule" class="headerlink" title="the MAP(maximum a posteriori ) decision rule"></a>the MAP(maximum a posteriori ) decision rule</h3>最大后验概率决策规则是贝叶斯决策理论的基本决策规则。</li>
</ul>
<h4 id="Bayes-decision-rule-MAP-decision-rule-可写成："><a href="#Bayes-decision-rule-MAP-decision-rule-可写成：" class="headerlink" title="Bayes decision rule (MAP decision rule)可写成："></a>Bayes decision rule (MAP decision rule)可写成：</h4><script type="math/tex; mode=display">P(w_j\mid x) =\frac{p(x\mid \omega_j) P(\omega_j)}{p(x)}, \quad where \quad p(x) = \sum_{j=1}^2p(x\mid \omega_j)P(\omega_j)</script><p>即，</p>
<script type="math/tex; mode=display">Posterior = \frac{Likelihood * Prior}{Evidence}</script><h4 id="The-MAP-decision-rule-判别"><a href="#The-MAP-decision-rule-判别" class="headerlink" title="The MAP decision rule 判别"></a>The MAP decision rule 判别</h4><p>以二分类问题为例：</p>
<script type="math/tex; mode=display">if\quad P(\omega_1\mid x) > P(\omega_2\mid x), then\quad x = w_1</script><script type="math/tex; mode=display">if\quad P(\omega_2\mid x) > P(\omega_1\mid x), then\quad x = w_2</script><h3 id="Likelihood-ratio-test-and-ML-rule"><a href="#Likelihood-ratio-test-and-ML-rule" class="headerlink" title="Likelihood ratio test and ML rule"></a>Likelihood ratio test and ML rule</h3><p>将MAP决策规则写成如下形式：</p>
<script type="math/tex; mode=display">if \quad p(x\mid \omega_1)P(\omega_1) > p(x\mid \omega_2) P(\omega_2)\quad x = \omega_1, \quad else \quad x = \omega_2.</script><p>则似然比检验公式为</p>
<script type="math/tex; mode=display">l(x)=\frac{p\left(x \mid \omega_{1}\right)}{p\left(x \mid \omega_{2}\right)} \stackrel{\omega_{1}}{>} \frac{P\left(\omega_{2}\right)}{P\left(\omega_{1}\right)}=\theta</script><script type="math/tex; mode=display">l(x)=\frac{p\left(x \mid \omega_{1}\right)}{p\left(x \mid \omega_{2}\right)} \stackrel{\omega_{2}}{<} \frac{P\left(\omega_{2}\right)}{P\left(\omega_{1}\right)}=\theta</script><p>两种特殊的情况</p>
<ul>
<li>如果$p(x\mid \omega_1) = p(x\mid \omega_2)$，则决策只取决于先验概率；</li>
<li>如果$P(\omega_1) = P(\omega_2)$，则决策取决于似然概率，即<strong>最大似然概率决策规则</strong>(ML, Maximum Likehood, decision rule)。</li>
</ul>
<h3 id="Basic-conepts-of-error-probability"><a href="#Basic-conepts-of-error-probability" class="headerlink" title="Basic conepts of error probability"></a>Basic conepts of error probability</h3><p><img src="/2021/11/10/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%AF%B9%E6%8A%97%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/error_prob.png" alt="error_prob"></p>
<p>$x = x_B$是最优的决策边界，此时决策具有最小误差，称作<strong>Bayes error</strong>；如果选择$x = x^*$作为决策边界，会产生额外的误差。<br>对于二分类问题，</p>
<script type="math/tex; mode=display">P(error) = P\{x\in R_2, \omega_1 \} + P\{ x\in R_1, \omega_2 \}=P(\omega_1)\int_{R_2}p(x\mid \omega_1)dx + P(\omega_2) \int_{R_1}p(x\mid \omega_2)dx</script><script type="math/tex; mode=display">P(correct) = \sum_{i=1}^c P\{x\in R_i, \omega_i\}</script><script type="math/tex; mode=display">P(error) = 1 - P(correct)</script><h3 id="Discriminant-functions-and-decision-regions"><a href="#Discriminant-functions-and-decision-regions" class="headerlink" title="Discriminant functions and decision regions"></a>Discriminant functions and decision regions</h3><h4 id="Discriminant-functions-g-i-x"><a href="#Discriminant-functions-g-i-x" class="headerlink" title="Discriminant functions $g_i(x)$"></a>Discriminant functions $g_i(x)$</h4><p>如果对任意的$i \neq j$，都有 $g_i(x) &gt; g_j(x)$， 则$x\in R_i$ .</p>
<h4 id="Decision-Boundaries"><a href="#Decision-Boundaries" class="headerlink" title="Decision Boundaries"></a>Decision Boundaries</h4><p>决策边界在$g_i(x) = g_j(x)$处。</p>
<h3 id="From-error-probability-to-risk"><a href="#From-error-probability-to-risk" class="headerlink" title="From error probability to risk"></a>From error probability to risk</h3><p>在某些时候，不同的决策具有不同的风险。</p>
<h4 id="Minimum-risk-theory"><a href="#Minimum-risk-theory" class="headerlink" title="Minimum risk theory"></a>Minimum risk theory</h4><ul>
<li>问题定义<br>Data classes: $\Omega = \{ \omega_1, \omega_2, \ldots, \omega_c \}$<br>Actions/Decision; $A = \{ \alpha_1, \alpha_2, \ldots, \alpha_c \}$</li>
<li><strong>损失矩阵</strong>定义了当把$\omega_j$的样本分为$\alpha_i$时的损失：<br>$\Lambda=\left[\begin{array}{cccc}<br>\lambda\left(\alpha_{1} \mid \omega_{1}\right) &amp; \lambda\left(\alpha_{1} \mid \omega_{2}\right) &amp; \cdots &amp; \lambda\left(\alpha_{1} \mid \omega_{c}\right) \\<br>\lambda\left(\alpha_{2} \mid \omega_{1}\right) &amp; \lambda\left(\alpha_{2} \mid \omega_{2}\right) &amp; \cdots &amp; \lambda\left(\alpha_{2} \mid \omega_{c}\right) \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>\lambda\left(\alpha_{a} \mid \omega_{1}\right) &amp; \lambda\left(\alpha_{a} \mid \omega_{1}\right) &amp; \cdots &amp; \lambda\left(\alpha_{a} \mid \omega_{c}\right)<br>\end{array}\right]$</li>
</ul>
<p>决策为$\alpha_j$时的条件风险为：</p>
<script type="math/tex; mode=display">R\left(\alpha_{i} \mid \mathbf{x}\right)=\sum_{j=1}^{c} \lambda\left(\alpha_{i} \mid \omega_{j}\right) P\left(\omega_{j} \mid \mathbf{x}\right)=\underset{\omega \in \Omega}{E}\left\{\lambda\left(\alpha_{i} \mid \omega\right) \mid \mathbf{x}\right\}</script><p>因此， 最小风险决策准则为：</p>
<script type="math/tex; mode=display">\mathbf{x} \rightarrow \alpha_{i} \Leftrightarrow R\left(\alpha_{i} \mid \mathbf{x}\right)<R\left(\alpha_{j} \mid \mathbf{x}\right) \forall \mathrm{i} \neq \mathrm{j}, \mathrm{i}=1, \ldots, \mathrm{a}</script><p>使用似然比判断分类结果：</p>
<script type="math/tex; mode=display">\mathrm{x} \in \omega_{1} \text { if } l(\mathrm{x})=\frac{p\left(x / \omega_{1}\right)}{p\left(x / \omega_{2}\right)}>\frac{\left(\lambda_{12}-\lambda_{22}\right)}{\left(\lambda_{21}-\lambda_{11}\right)} \frac{P\left(\omega_{2}\right)}{P\left(\omega_{1}\right)}=\theta</script><p>在有些情况下，使用简单的0-1损失矩阵：</p>
<script type="math/tex; mode=display">\lambda\left(\alpha_{i}, \omega_{j}\right)=\left\{\begin{array}{ll}
0 & i=j \\
1 & i \neq j
\end{array} \quad i, j=1, \ldots, c\right.</script><p>此时的风险损失等于错误概率</p>
<script type="math/tex; mode=display">\begin{aligned}
R\left(\alpha_{i} \mid x\right)&=\sum_{j=1}^{c} \lambda\left(\alpha_{i} \mid \omega_{j}\right) P\left(\omega_{j} \mid x\right) \\
&=\sum_{j \neq i} P\left(\omega_{j} \mid x\right)=1-P\left(\omega_{i} \mid x\right)
\end{aligned}</script><h3 id="Decision-with-reject-option"><a href="#Decision-with-reject-option" class="headerlink" title="Decision with reject option"></a>Decision with reject option</h3><p>在某些情况，即使能够到达最小的贝叶斯误差，某些错误决策也是不可能接受的。一个限制决策错误简单的方式是不做决定或者推迟决定。</p>
<p>带拒绝项的分类问题需要一个额外的类别。</p>
<h4 id="Error-reject-trade-off-and-Chow’s-rule-equal-costs"><a href="#Error-reject-trade-off-and-Chow’s-rule-equal-costs" class="headerlink" title="Error-reject trade-off and Chow’s rule (equal costs)"></a>Error-reject trade-off and Chow’s rule (equal costs)</h4><ul>
<li>Chow’s rule<br><img src="/2021/11/10/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%AF%B9%E6%8A%97%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/rejection_rule.png" alt="Chow&#39;s rule"><br>$\lambda_r$ is reject cost, $\lambda_e$ is error cost,<br>$\lambda_c$ is correct classification cost(通常为0)<br>Cow’s rule通过对分类器没有把握的样本不做决策最小化误差。</li>
</ul>
<p>不等式$\max [P(\omega_i\mid x)] &lt; T$决定了拒绝决策边界(reject region)</p>
<h2 id="Generative-machine-learning-models-the-Gaussian-Pattern-Classifiers"><a href="#Generative-machine-learning-models-the-Gaussian-Pattern-Classifiers" class="headerlink" title="Generative machine learning models: the Gaussian Pattern Classifiers"></a>Generative machine learning models: the Gaussian Pattern Classifiers</h2><h3 id="Gaussian-probabilistic-model"><a href="#Gaussian-probabilistic-model" class="headerlink" title="Gaussian probabilistic model"></a>Gaussian probabilistic model</h3><p>如果一个模型的后验概率为$p(x\mid \omega_i)$满足高斯分布，则问题简化为估计分布的两个参数$\mu$和$\sigma$。</p>
<h2 id="From-linear-classifiers-to-deep-neural-networks"><a href="#From-linear-classifiers-to-deep-neural-networks" class="headerlink" title="From linear classifiers to deep neural networks"></a>From linear classifiers to deep neural networks</h2><p>决策边界垂直于向量$\mu_1 - \mu_2$， 并通过均值点$\frac{\mu_1+\mu_2}{2}$</p>
<h3 id="Learning-asa-an-optimization-problem"><a href="#Learning-asa-an-optimization-problem" class="headerlink" title="Learning asa an optimization problem"></a>Learning asa an optimization problem</h3><p>估计非线性分类函数的参数$\omega , \theta$:</p>
<script type="math/tex; mode=display">\omega^*, b^* = \argmin_{\omega, b}\frac{1}{n}\sum_{i=1}^n\mathcal{l}(y_i, f(x_i)) + \lambda \Omega(\omega)</script><p>$\frac{1}{n}\sum_{i=1}^n\mathcal{l}(y_i, f(x_i)) $为损失项$L(D, \theta)$, $\lambda \Omega(\omega)$为正则项， $\lambda$为正则超参数。<br>加入正则项以避免过拟合及使损失函数更加平滑，超参数$\lambda$平衡训练损失和正则项。<br>简化的损失函数形式为$\theta^<em> = \argmin_{\theta}L(D, \theta)=\frac{1}{n}\sum_{i=1}^n\mathcal{l}(y_i, f(x_i; \theta))$，训练过程中找到合适的$\theta$网络参数，使损失函数最小，即<strong>经验风险最小化(Empirical Risk Minimization(ERM))</strong>。<br><em>*Zero-One loss function</em></em>难以优化， NP hard 问题。</p>
<h3 id="使用凸函数作为损失函数"><a href="#使用凸函数作为损失函数" class="headerlink" title="使用凸函数作为损失函数"></a>使用凸函数作为损失函数</h3><p><strong>Hinge Loss</strong> 通过$\mathcal{l}(y, f(x;\theta)) = \max (0, 1-yf)$计算。<br><strong>Exponential loss</strong>: $\mathcal{l}(y, f(x)) = e^{-yf}$<br><strong>Logistic loss</strong>: $\mathcal{l}(y, f(x))=\log_2(1+e^{-yf})$<br><img src="/2021/11/10/%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%AF%B9%E6%8A%97%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/losses.png" alt="losses"></p>
<h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><p>$L(D, \theta)=\frac{1}{n}\sum_{i=1}^n\mathcal{l}(y_i, f(x_i; \theta))$, $\nabla_\theta L = \frac{1}{n}\sum_{i=1}^n\nabla_\theta \mathcal{l}(y_i, f(x_i; \theta))$</p>
<h3 id="Gradient-descent-on-quadratic-objectives"><a href="#Gradient-descent-on-quadratic-objectives" class="headerlink" title="Gradient descent on quadratic objectives"></a>Gradient descent on quadratic objectives</h3><p>如果损失函数是二次或者非凸的，它可能很难收敛，因此可以对损失函数$L(D, \theta)$在$\theta$附近使用泰勒公式二次展开：</p>
<script type="math/tex; mode=display">L(\theta_{k+1}) \approxeq L(\theta_k) + \nabla L(\theta_k)(\theta_{k+1}-\theta_k)+\frac{1}{2}(\theta_{k+1}-\theta_k)^TH_k(\theta_{k+1}-\theta_k)</script><p>$H_k = \nabla^2_{\theta, \theta}L(\theta_k)$为Hessian 矩阵。</p>
<h4 id="Steepest-Descent-with-Exact-Step"><a href="#Steepest-Descent-with-Exact-Step" class="headerlink" title="Steepest Descent with Exact Step"></a>Steepest Descent with Exact Step</h4><p>如果使用$\theta_{k+1} = \theta_k - \eta_k \nabla L(\theta_k)$更新规则。</p>
<script type="math/tex; mode=display">L(\theta_{k+1})\approxeq L(\theta_k)-\eta_k\| \nabla L(\theta_k)\|^2+\frac{1}{2}\eta_k^2\nabla L(\theta_k)^TH_k\nabla L(\theta_k)</script><p>每一册迭代的最优步长为 $\eta_k = \frac{| \nabla L(\theta_k)|^2}{\nabla L(\theta_k)^TH_k\nabla L(\theta_k)}$.</p>
<h4 id="Newton-Raphson-Method"><a href="#Newton-Raphson-Method" class="headerlink" title="Newton-Raphson Method"></a>Newton-Raphson Method</h4><script type="math/tex; mode=display">L(\theta_{k+1}) \approxeq L(\theta_k)+\nabla L(\theta_k)(\theta_{k+1}-\theta_k)+\frac{1}{2}(\theta_{k+1}-\theta_k)^TH_k(\theta_{k+1}-\theta_k)</script><p>通过$\nabla L(\theta_k) + H_k(\theta_{k+1}-\theta_k) = 0$， 得到newton-raphson的更新策略$\theta_{k+1} = \theta_k - H_k^{-1}\nabla L(\theta_k)$<br> New-Raphson具有更快的收敛速度，但是hessian矩阵的计算代价过高。</p>
]]></content>
      <categories>
        <category>ClassNotes</category>
      </categories>
      <tags>
        <tag>统计机器学习</tag>
        <tag>课程笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>ssh 登录服务器相关</title>
    <url>/2022/04/10/ssh%E7%99%BB%E5%BD%95%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9B%B8%E5%85%B3/</url>
    <content><![CDATA[<h2 id="ssh-登录服务器使用别名"><a href="#ssh-登录服务器使用别名" class="headerlink" title="ssh 登录服务器使用别名"></a>ssh 登录服务器使用别名</h2><p>在 .ssh文件夹中创建config文件，写法如下<br><img src="/2022/04/10/ssh%E7%99%BB%E5%BD%95%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9B%B8%E5%85%B3/config.PNG" alt></p>
<h2 id="配置免密登录远程服务器"><a href="#配置免密登录远程服务器" class="headerlink" title="配置免密登录远程服务器"></a>配置免密登录远程服务器</h2><h3 id="远程服务器端设置"><a href="#远程服务器端设置" class="headerlink" title="远程服务器端设置"></a>远程服务器端设置</h3><p>修改 /etc/ssh/sshd_config 文件<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 是否允许 root 远程登录</span><br><span class="line">PermitRootLogin yes</span><br><span class="line"></span><br><span class="line"># 密码登录是否打开</span><br><span class="line">PasswordAuthentication yes</span><br><span class="line"></span><br><span class="line"># 开启公钥认证</span><br><span class="line">RSAAuthentication yes # 这个参数可能没有 没关系</span><br><span class="line">PubkeyAuthentication yes</span><br><span class="line"></span><br><span class="line"># 存放登录用户公钥的文件位置</span><br><span class="line"># 位置就是登录用户名的家目录下的 .ssh</span><br><span class="line"># root 就是 /root/.ssh</span><br><span class="line"># foo 就是 /home/foo/.ssh</span><br><span class="line">AuthorizedKeysFile .ssh/authorized_keys</span><br></pre></td></tr></table></figure></p>
<p>重启服务 <code>service sshd restart</code></p>
<h3 id="添加公钥"><a href="#添加公钥" class="headerlink" title="添加公钥"></a>添加公钥</h3><p>将本地机器的公钥 id_rsa.pub 内容添加到服务器的 ~/.ssh/authorized_keys 中。</p>
<blockquote>
<p>直接将多个公钥添加到authorized_keys文件中可实现多台本地电脑免密登录</p>
</blockquote>
]]></content>
      <categories>
        <category>ssh</category>
      </categories>
      <tags>
        <tag>ssh</tag>
      </tags>
  </entry>
  <entry>
    <title>数据结构</title>
    <url>/2021/07/14/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</url>
    <content><![CDATA[<h1 id><a href="#" class="headerlink" title=" "></a> </h1>]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
</search>
